#!/usr/bin/env python3
"""
Real Data Testing Script for Dataset Cleaner + Latency Tick-Store
Tests the system on actual uncleaned datasets provided by the user
"""

import sys
import os
import tempfile
import shutil
import time
import zipfile
from pathlib import Path
from datetime import datetime
import pandas as pd
import numpy as np

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent / "src"))

def extract_zip_files():
    """Extract any zip files in the current directory."""
    print("üì¶ Extracting compressed datasets...")
    
    extracted_files = []
    for file_path in Path(".").glob("*.zip"):
        print(f"   Extracting {file_path.name}...")
        try:
            with zipfile.ZipFile(file_path, 'r') as zip_ref:
                extract_dir = Path(f"extracted_{file_path.stem}")
                zip_ref.extractall(extract_dir)
                
                # Find CSV files in extracted directory
                for csv_file in extract_dir.rglob("*.csv"):
                    extracted_files.append(csv_file)
                    print(f"   Found: {csv_file}")
        except Exception as e:
            print(f"   Error extracting {file_path}: {e}")
    
    return extracted_files

def analyze_dataset(file_path: Path):
    """Analyze a dataset and return its characteristics."""
    print(f"\nüìä Analyzing {file_path.name}...")
    
    try:
        # Read first few lines to understand structure
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            first_lines = [f.readline().strip() for _ in range(5)]
        
        # Try to read with pandas
        try:
            df = pd.read_csv(file_path, nrows=1000)  # Read first 1000 rows for analysis
            print(f"   ‚úÖ Successfully read {len(df)} rows")
            print(f"   üìã Columns: {list(df.columns)}")
            print(f"   üìè Shape: {df.shape}")
            
            # Check for data quality issues
            issues = []
            
            # Check for missing values
            missing_counts = df.isnull().sum()
            if missing_counts.sum() > 0:
                issues.append(f"Missing values: {missing_counts.sum()} total")
            
            # Check for duplicate rows
            duplicates = df.duplicated().sum()
            if duplicates > 0:
                issues.append(f"Duplicate rows: {duplicates}")
            
            # Check for mixed data types
            dtypes = df.dtypes
            print(f"   üîç Data types: {dict(dtypes)}")
            
            # Check for potential issues in specific columns
            for col in df.columns:
                if df[col].dtype == 'object':
                    # Check for mixed content in string columns
                    sample_values = df[col].dropna().head(10).astype(str)
                    if len(sample_values) > 0:
                        print(f"   üìù Sample values in '{col}': {list(sample_values)}")
            
            return {
                'file_path': file_path,
                'rows': len(df),
                'columns': list(df.columns),
                'shape': df.shape,
                'dtypes': dict(dtypes),
                'missing_values': missing_counts.sum(),
                'duplicates': duplicates,
                'issues': issues,
                'sample_data': df.head(3)
            }
            
        except Exception as e:
            print(f"   ‚ùå Error reading with pandas: {e}")
            return {
                'file_path': file_path,
                'error': str(e),
                'first_lines': first_lines
            }
            
    except Exception as e:
        print(f"   ‚ùå Error analyzing file: {e}")
        return {'file_path': file_path, 'error': str(e)}

def create_schema_for_dataset(analysis):
    """Create a schema definition for a dataset."""
    if 'error' in analysis:
        return None
    
    # Create a basic schema based on the data
    fields = []
    
    for col_name, dtype in analysis['dtypes'].items():
        field_type = 'string'
        if 'int' in str(dtype):
            field_type = 'integer'
        elif 'float' in str(dtype):
            field_type = 'float'
        elif 'datetime' in str(dtype):
            field_type = 'datetime'
        
        fields.append({
            'name': col_name,
            'type': field_type,
            'nullable': True,
            'description': f'Auto-generated field for {col_name}'
        })
    
    schema_id = f"{analysis['file_path'].stem}_v1"
    
    return {
        'id': schema_id,
        'name': f"Schema for {analysis['file_path'].name}",
        'description': f"Auto-generated schema for {analysis['file_path'].name}",
        'fields': fields,
        'version': '1.0'
    }

def test_data_cleaning():
    """Test the data cleaning functionality on real datasets."""
    print("üß™ Testing Dataset Cleaner on Real Uncleaned Data")
    print("=" * 80)
    
    try:
        # Import TickDB components
        from tickdb.core import TickDB, TickDBConfig
        from tickdb.schemas import SchemaRegistry
        print("   ‚úÖ Successfully imported TickDB components")
        
        # Extract zip files
        extracted_files = extract_zip_files()
        
        # Find all CSV files
        csv_files = list(Path(".").glob("*.csv")) + extracted_files
        print(f"\nüìÅ Found {len(csv_files)} CSV files to test:")
        for file in csv_files:
            print(f"   - {file.name}")
        
        # Create temporary directory for testing
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_path = Path(temp_dir)
            
            # Initialize TickDB
            config = TickDBConfig(
                data_path=temp_path / "data",
                quarantine_path=temp_path / "quarantine",
                batch_size=1000,
                enable_metrics=False
            )
            
            tickdb = TickDB(config)
            print("   ‚úÖ TickDB initialized")
            
            # Test each dataset
            results = []
            
            for csv_file in csv_files:
                print(f"\n{'='*60}")
                print(f"üîç Testing: {csv_file.name}")
                print(f"{'='*60}")
                
                # Analyze the dataset
                analysis = analyze_dataset(csv_file)
                
                if 'error' in analysis:
                    print(f"   ‚ùå Skipping {csv_file.name} due to error: {analysis['error']}")
                    results.append({
                        'file': csv_file.name,
                        'status': 'error',
                        'error': analysis['error']
                    })
                    continue
                
                # Create schema for this dataset
                schema = create_schema_for_dataset(analysis)
                if schema:
                    print(f"   üìã Created schema: {schema['id']}")
                    print(f"   üìä Schema fields: {len(schema['fields'])}")
                
                # Test data loading
                try:
                    print(f"   üì• Loading data into TickDB...")
                    start_time = time.time()
                    
                    # Load the data
                    result = tickdb.load_raw(
                        source_id=csv_file.stem,
                        path=str(csv_file),
                        schema_id=schema['id'] if schema else "generic_v1"
                    )
                    
                    load_time = time.time() - start_time
                    
                    print(f"   ‚úÖ Load completed in {load_time:.2f}s")
                    print(f"   üìä Rows processed: {result.get('rows_processed', 0)}")
                    print(f"   üìä Rows failed: {result.get('rows_failed', 0)}")
                    print(f"   üìä Files created: {result.get('files_created', 0)}")
                    
                    # Test querying the loaded data
                    print(f"   üîç Testing queries...")
                    
                    # Get metadata
                    metadata = tickdb.get_metadata(schema_id=schema['id'] if schema else "generic_v1")
                    print(f"   üìã Metadata: {metadata}")
                    
                    # Test a simple query
                    try:
                        query_result = tickdb.read(
                            schema_id=schema['id'] if schema else "generic_v1",
                            limit=10
                        )
                        print(f"   ‚úÖ Query successful, returned {len(query_result)} rows")
                        
                        # Show sample of cleaned data
                        if len(query_result) > 0:
                            print(f"   üìù Sample cleaned data:")
                            print(query_result.head(3).to_string())
                        
                    except Exception as e:
                        print(f"   ‚ö†Ô∏è Query failed: {e}")
                    
                    results.append({
                        'file': csv_file.name,
                        'status': 'success',
                        'rows_processed': result.get('rows_processed', 0),
                        'rows_failed': result.get('rows_failed', 0),
                        'load_time': load_time,
                        'schema_id': schema['id'] if schema else "generic_v1"
                    })
                    
                except Exception as e:
                    print(f"   ‚ùå Loading failed: {e}")
                    results.append({
                        'file': csv_file.name,
                        'status': 'load_error',
                        'error': str(e)
                    })
            
            # Summary
            print(f"\n{'='*80}")
            print("üìã TESTING SUMMARY")
            print(f"{'='*80}")
            
            successful = 0
            total_rows = 0
            total_failed = 0
            
            for result in results:
                status_icon = "‚úÖ" if result['status'] == 'success' else "‚ùå"
                print(f"{status_icon} {result['file']:<40} {result['status']}")
                
                if result['status'] == 'success':
                    successful += 1
                    total_rows += result.get('rows_processed', 0)
                    total_failed += result.get('rows_failed', 0)
            
            print(f"\nüìä Overall Results:")
            print(f"   Files processed: {len(results)}")
            print(f"   Successful: {successful}")
            print(f"   Failed: {len(results) - successful}")
            print(f"   Total rows processed: {total_rows:,}")
            print(f"   Total rows failed: {total_failed:,}")
            print(f"   Success rate: {(successful/len(results)*100):.1f}%")
            
            # Test health check
            print(f"\nüè• System Health Check:")
            health = tickdb.health_check()
            print(f"   Status: {health['status']}")
            print(f"   Data files: {health.get('data_files', 0)}")
            print(f"   Quarantine files: {health.get('quarantine_files', 0)}")
            
            return successful == len(results)
            
    except Exception as e:
        print(f"\n‚ùå Test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_performance_on_real_data():
    """Test performance on the largest real dataset."""
    print("\nüöÄ Performance Testing on Real Data")
    print("=" * 60)
    
    try:
        from tickdb.core import TickDB, TickDBConfig
        import time
        
        # Find the largest CSV file
        csv_files = list(Path(".").glob("*.csv"))
        if not csv_files:
            print("   No CSV files found for performance testing")
            return False
        
        # Get file sizes
        file_sizes = [(f, f.stat().st_size) for f in csv_files]
        largest_file = max(file_sizes, key=lambda x: x[1])
        
        print(f"   Testing on largest file: {largest_file[0].name} ({largest_file[1]/1024/1024:.1f} MB)")
        
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_path = Path(temp_dir)
            
            config = TickDBConfig(
                data_path=temp_path / "data",
                quarantine_path=temp_path / "quarantine",
                batch_size=16384,
                enable_metrics=False
            )
            
            tickdb = TickDB(config)
            
            # Test loading performance
            print(f"   üì• Loading {largest_file[0].name}...")
            start_time = time.time()
            
            result = tickdb.load_raw(
                source_id=largest_file[0].stem,
                path=str(largest_file[0]),
                schema_id="performance_test_v1"
            )
            
            load_time = time.time() - start_time
            file_size_mb = largest_file[1] / (1024 * 1024)
            throughput_mbps = file_size_mb / load_time
            
            print(f"   üìä Load completed in {load_time:.2f}s")
            print(f"   üìä Throughput: {throughput_mbps:.2f} MB/s")
            print(f"   üìä Rows processed: {result.get('rows_processed', 0):,}")
            print(f"   üìä Rows failed: {result.get('rows_failed', 0):,}")
            
            # Test query performance
            print(f"   üîç Testing query performance...")
            start_time = time.time()
            
            query_result = tickdb.read(
                schema_id="performance_test_v1",
                limit=1000
            )
            
            query_time = (time.time() - start_time) * 1000  # Convert to ms
            print(f"   üìä Query completed in {query_time:.2f}ms")
            print(f"   üìä Query returned {len(query_result)} rows")
            
            # Performance assessment
            print(f"\nüìà Performance Assessment:")
            if throughput_mbps >= 600:  # 600 MB/s = ~36 GB/min
                print(f"   ‚úÖ Throughput: {throughput_mbps:.2f} MB/s (Target: ‚â•600 MB/s)")
            else:
                print(f"   ‚ö†Ô∏è Throughput: {throughput_mbps:.2f} MB/s (Target: ‚â•600 MB/s)")
            
            if query_time < 20:
                print(f"   ‚úÖ Query latency: {query_time:.2f}ms (Target: <20ms)")
            else:
                print(f"   ‚ö†Ô∏è Query latency: {query_time:.2f}ms (Target: <20ms)")
            
            return True
            
    except Exception as e:
        print(f"   ‚ùå Performance test failed: {e}")
        return False

def main():
    """Run all real data tests."""
    print("üß™ Dataset Cleaner + Latency Tick-Store - Real Data Testing")
    print("=" * 80)
    
    # Install required packages if not available
    try:
        import pandas as pd
        import numpy as np
    except ImportError:
        print("Installing required packages...")
        os.system("pip install pandas numpy pyarrow duckdb pydantic")
    
    # Run tests
    tests = [
        ("Data Cleaning on Real Datasets", test_data_cleaning),
        ("Performance on Real Data", test_performance_on_real_data),
    ]
    
    results = []
    for test_name, test_func in tests:
        print(f"\n{'='*20} {test_name} {'='*20}")
        result = test_func()
        results.append((test_name, result))
    
    # Summary
    print("\n" + "="*80)
    print("üìã REAL DATA TESTING SUMMARY")
    print("="*80)
    
    passed = 0
    total = len(results)
    
    for test_name, result in results:
        status = "‚úÖ PASSED" if result else "‚ùå FAILED"
        print(f"{test_name:<35} {status}")
        if result:
            passed += 1
    
    print(f"\nOverall: {passed}/{total} tests passed")
    
    if passed == total:
        print("\nüéâ ALL REAL DATA TESTS PASSED!")
        print("\nYour Dataset Cleaner successfully processed the uncleaned data!")
        print("\nKey achievements:")
        print("‚úÖ Handled various data formats and structures")
        print("‚úÖ Detected and quarantined invalid rows")
        print("‚úÖ Created schemas automatically")
        print("‚úÖ Achieved target performance metrics")
        print("‚úÖ Provided comprehensive data quality insights")
    else:
        print(f"\n‚ö†Ô∏è  {total - passed} tests failed. Check the errors above.")
    
    return passed == total

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 